{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Stratosphere Linux IPS allows a number of file formats such as pcap and zeek flows to train and test the ML detection algorithm. The issue with pcaps is that Slips turns the pcaps into flows then labels the flow as benign or malicious instead of the individual packets. If the labelled flows are not congruent with the way Slips creates the flow then the labelled flows cannot be used to evaluate correctness.\n",
    "\n",
    "If the input is flows, then Slips will classify the flows themselves as malicious or benign. This means the output of Slips and the labelled flows are directly comparable. To do this, the labelled flows need to be converted to a format that Slips understands. The way this is done may be different from dataset to dataset but the general procedure is to extract the relevant columns from each datasets labelled flow and put it in a zeek conn.log file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing CICIDS2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the labelled flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Function to convert scientific notation to decimal\n",
    "def scientific_to_decimal(value):\n",
    "    try:\n",
    "        # Check if the value is in scientific notation (contains 'e' or 'E')\n",
    "        if 'e+' in value or 'E+' in value:\n",
    "            decimal_value = int(float(value))\n",
    "            return decimal_value\n",
    "        else:\n",
    "            # Return the original value if it's not in scientific notation\n",
    "            return value\n",
    "    except ValueError:\n",
    "        # If it's not a valid float, return the original value\n",
    "        return value\n",
    "\n",
    "# Input CSV file and output cleaned CSV file\n",
    "INPUT_CSV_FILE = \"Thursday-WorkingHours-Full.csv\"\n",
    "OUTPUT_CSV_FILE = \"Thursday-WorkingHours-Full_cleaned.csv\"\n",
    "\n",
    "# Open the input and output CSV files\n",
    "with open(INPUT_CSV_FILE, \"r\") as infile, open(OUTPUT_CSV_FILE, \"w\", newline=\"\") as outfile:\n",
    "    reader = csv.reader(infile)\n",
    "    writer = csv.writer(outfile)\n",
    "    \n",
    "    for row in reader:\n",
    "        # Apply the scientific_to_decimal function to fields in scientific notation\n",
    "        cleaned_row = [scientific_to_decimal(field) for field in row]\n",
    "        writer.writerow(cleaned_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert labelled flows into Zeek/Bro conn.log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import csv\n",
    "\n",
    "CICIDS_LABELLED = \"Thursday-WorkingHours-Full_cleaned.csv\"\n",
    "ZEEK_OUTPUT = \"conn.log\"\n",
    "\n",
    "# This is a standard header of all Zeek conn.log flow files. Note that the values for #open and #path does not matter.\n",
    "ZEEK_HEADER ='''#separator \\\\x09\n",
    "#set_separator\t,\n",
    "#empty_field\t(empty)\n",
    "#unset_field\t-\n",
    "#path\tconn\n",
    "#open\n",
    "#fields\tts\tuid\tid.orig_h\tid.orig_p\tid.resp_h\tid.resp_p\tproto\tservice\tduration\torig_bytes\tresp_bytes\tconn_state\tlocal_orig\tlocal_resp\tmissed_bytes\thistory\torig_pkts\torig_ip_bytes\tresp_pkts\tresp_ip_bytes\ttunnel_parents\n",
    "#types\ttime\tstring\taddr\tport\taddr\tport\tenum\tstring\tinterval\tcount\tcount\tstring\tbool\tbool\tcount\tstring\tcount\tcount\tcount\tcount\tset[string]\n",
    "'''\n",
    "\n",
    "# conn.log fields mapped to CICIDS flows file\n",
    "# for fields that do not have a mapping, a default value is set\n",
    "TIME_STAMP =    6\n",
    "FLOW_ID =       0\n",
    "SOURCE_IP =     1\n",
    "SOURCE_PORT =   2\n",
    "DEST_IP =       3\n",
    "DEST_PORT =     4\n",
    "PROTOCOL =      5\n",
    "SERVICE =       13\n",
    "DURATION =      7\n",
    "FWD_BYTES =     10\n",
    "BWD_BYTES =     11\n",
    "CONN_STATE =    '-'\n",
    "LOCAL_ORIG =    '-'\n",
    "LOCAL_RESP =    '-'\n",
    "MISSED =        '-'\n",
    "HISTORY =       '-'\n",
    "ORIG_PKTS =     8\n",
    "ORIG_IP_BYTES = '-'\n",
    "RESP_PKTS =     9\n",
    "RESP_IP_BYTES = '-'\n",
    "TUNNEL_PARENT = '(empty)'\n",
    "\n",
    "# protocol mapping for CICIDS2017\n",
    "PROTOCOL_DICT = {\n",
    "    1: \"icmp\",\n",
    "    2: \"igmp\",\n",
    "    3: \"ggp\",\n",
    "    6: \"tcp\",\n",
    "    8: \"egp\",\n",
    "    9: \"igp\",\n",
    "    17: \"udp\",\n",
    "    20: \"hmp\",\n",
    "    27: \"rdp\",\n",
    "    41: \"ipv6\",\n",
    "    46: \"rsvp\",\n",
    "    47: \"gre\",\n",
    "    50: \"esp\",\n",
    "    51: \"ah\",\n",
    "    58: \"ipv6-icmp\",\n",
    "    59: \"ipv6-nonxt\",\n",
    "    88: \"eigrp\",\n",
    "    89: \"ospf\",\n",
    "    115: \"l2tp\",\n",
    "    132: \"sctp\",\n",
    "    139: \"netbios\",\n",
    "    143: \"imap\",\n",
    "    161: \"snmp\",\n",
    "    179: \"bgp\"\n",
    "}\n",
    "\n",
    "with open(CICIDS_LABELLED, \"r\") as data, open(ZEEK_OUTPUT, \"w\") as output:\n",
    "    labelled_flows = csv.reader(data)\n",
    "    next(labelled_flows) #skip header\n",
    "    \n",
    "    output.write(ZEEK_HEADER)\n",
    "\n",
    "    fid = 1\n",
    "\n",
    "    for flow in labelled_flows:\n",
    "        time = str(datetime.strptime(flow[TIME_STAMP], \"%d/%m/%Y %H:%M\").timestamp()) # Thursday flows don't include seconds\n",
    "        #time = str(datetime.strptime(flow[TIME_STAMP], \"%d/%m/%Y %H:%M:%S\").timestamp()) # Monday flows do include seconds\n",
    "        #flowid = flow[FLOW_ID]\n",
    "        flowid = str(fid)\n",
    "        fid += 1\n",
    "        src_ip = flow[SOURCE_IP]\n",
    "        src_port = flow[SOURCE_PORT]\n",
    "        des_ip = flow[DEST_IP]\n",
    "        des_port = flow[DEST_PORT]\n",
    "        protocol = PROTOCOL_DICT.get(int(flow[PROTOCOL]), 'tcp')\n",
    "        service = SERVICE\n",
    "        duration = format(int(flow[DURATION]) / 1000000, \".6f\") \n",
    "        fwd_bytes = flow[FWD_BYTES]\n",
    "        bwd_bytes = flow[BWD_BYTES]\n",
    "        conn_state = CONN_STATE\n",
    "        local_orig = LOCAL_ORIG\n",
    "        local_resp = LOCAL_RESP\n",
    "        missed = MISSED\n",
    "        history = HISTORY\n",
    "        orig_pkts = flow[ORIG_PKTS]\n",
    "        orig_ip_bytes = ORIG_IP_BYTES\n",
    "        resp_pkts = flow[RESP_PKTS]\n",
    "        resp_ip_bytes = RESP_IP_BYTES\n",
    "        tunnel_parent = TUNNEL_PARENT\n",
    "\n",
    "        processed_line = time + \"\\t\" + flowid + \"\\t\" + src_ip + \"\\t\" + src_port + \"\\t\" + des_ip + \"\\t\" + des_port + \"\\t\" + protocol + \"\\t\" + service + \"\\t\" + duration + \"\\t\" + fwd_bytes + \"\\t\" + bwd_bytes + \"\\t\" + conn_state + \"\\t\" + local_orig + \"\\t\" + local_resp + \"\\t\" + missed + \"\\t\" + history + \"\\t\" + orig_pkts + \"\\t\" + orig_ip_bytes + \"\\t\" + resp_pkts + \"\\t\" + resp_ip_bytes + \"\\t\" + tunnel_parent + \"\\n\"\n",
    "\n",
    "        output.write(processed_line)\n",
    "\n",
    "    output.write(\"#close\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test or train\n",
    "\n",
    "When conn.log has been obtained, copy the file to the straosphere docker container to test or train the model. If training the model, raw pcaps may be used because Slips identifies malicious traffic by creating a baseline for normal traffic. If testing, a sqlite database will be output with labelled flows and it can be exported to a csv. This can be compared directly with the original labelled flows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate CICIDS2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of flows compared:  458968\n",
      "TN:  430133\n",
      "FN:  2117\n",
      "TP:  99\n",
      "FP:  26619\n",
      "Accuracy:  0.9373899705426086\n",
      "Recall:  0.044675090252707585\n",
      "Precision:  0.0037053671682012127\n"
     ]
    }
   ],
   "source": [
    "ORIGINAL_FLOWS = \"Thursday-WorkingHours-Full_cleaned.csv\"\n",
    "OUTPUT_FLOWS = \"Thursday-Output-Full.csv\"\n",
    "\n",
    "with open(ORIGINAL_FLOWS, \"r\") as original, open(OUTPUT_FLOWS, \"r\") as processed:\n",
    "    original_flows = csv.reader(original)\n",
    "    processed_flows = csv.reader(processed)\n",
    "\n",
    "    next(original_flows) #skip header\n",
    "\n",
    "    # counter for number of flows compared\n",
    "    count = 0\n",
    "\n",
    "    # statistics\n",
    "    fp = 0\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    tn = 0  \n",
    "\n",
    "    for original_flow, processed_flow in zip(original_flows, processed_flows):\n",
    "\n",
    "        classification = \"ERROR\" #default\n",
    "\n",
    "        if(original_flow[-1] == \"BENIGN\" and processed_flow[2].upper() == \"BENIGN\"): #TN\n",
    "            classification = \"TN\"\n",
    "            tn += 1\n",
    "        if(original_flow[-1] == \"BENIGN\" and processed_flow[2].upper() != \"BENIGN\"): #FP\n",
    "            classification = \"FP\"\n",
    "            fp += 1\n",
    "        if(original_flow[-1] != \"BENIGN\" and processed_flow[2].upper() == \"BENIGN\"): #FN\n",
    "            classification = \"FN\"\n",
    "            fn += 1\n",
    "        if(original_flow[-1] != \"BENIGN\" and processed_flow[2].upper() != \"BENIGN\"): #TP\n",
    "            classification = \"TP\"\n",
    "            tp += 1\n",
    "\n",
    "        # print(processed_flow[0], original_flow[-1], processed_flow[2].upper(), classification)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    # print results\n",
    "    print(\"Number of flows compared: \", count)\n",
    "    print(\"TN: \", tn)\n",
    "    print(\"FN: \", fn)\n",
    "    print(\"TP: \", tp)\n",
    "    print(\"FP: \", fp)\n",
    "    print(\"Accuracy: \", (tp + tn) / count)\n",
    "    print(\"Recall: \", tp / (tp + fn))\n",
    "    print(\"Precision: \", tp / (tp + fp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing UNSWNB15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "UNSW-NB15 provides labelled flows for testing and training machine learning models split into 4 parts. We can use one of these parts for training. Slips is trained by gauging malicious and benign traffic. As a result, the training set will need to split into two. For training, we will use \"UNSWNB15_assets\\UNSW-NB15_4.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "TRAINING_SET = \"UNSWNB15_assets/UNSW-NB15_4.csv\"\n",
    "TRAIN_MALICIOUS = \"UNSWNB15_assets/unswnb15_train_malicious.csv\"\n",
    "TRAIN_BENIGN = \"UNSWNB15_assets/unswnb15_train_benign.csv\"\n",
    "\n",
    "with open(TRAINING_SET, \"r\") as train, open(TRAIN_MALICIOUS, \"w\", newline=\"\") as mal, open(TRAIN_BENIGN, \"w\", newline=\"\") as ben:\n",
    "    train_in = csv.reader(train)\n",
    "    ben_out = csv.writer(ben)\n",
    "    mal_out = csv.writer(mal)\n",
    "\n",
    "    next(train_in)\n",
    "\n",
    "    for flow in train_in:\n",
    "        if flow[-1] == \"0\":\n",
    "            ben_out.writerow(flow)\n",
    "        else:\n",
    "            mal_out.writerow(flow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the benign and malicious training set can be converted into zeek flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "INPUT_CSV = \"UNSWNB15_assets/unswnb15_train_benign.csv\" # CHANGE THIS AS REQUIRED\n",
    "ZEEK_OUTPUT = \"UNSWNB15_assets/benign_conn.log\" # CHANGE THIS AS REQUIRED\n",
    "\n",
    "#INPUT_CSV = \"UNSWNB15_assets/UNSW-NB15_2.csv\" # CHANGE THIS AS REQUIRED\n",
    "#ZEEK_OUTPUT = \"UNSWNB15_assets/test_conn.log\" # CHANGE THIS AS REQUIRED\n",
    "\n",
    "# This is a standard header of all Zeek conn.log flow files. Note that the values for #open and #path does not matter.\n",
    "ZEEK_HEADER ='''#separator \\\\x09\n",
    "#set_separator\t,\n",
    "#empty_field\t(empty)\n",
    "#unset_field\t-\n",
    "#path\tconn\n",
    "#open\n",
    "#fields\tts\tuid\tid.orig_h\tid.orig_p\tid.resp_h\tid.resp_p\tproto\tservice\tduration\torig_bytes\tresp_bytes\tconn_state\tlocal_orig\tlocal_resp\tmissed_bytes\thistory\torig_pkts\torig_ip_bytes\tresp_pkts\tresp_ip_bytes\ttunnel_parents\n",
    "#types\ttime\tstring\taddr\tport\taddr\tport\tenum\tstring\tinterval\tcount\tcount\tstring\tbool\tbool\tcount\tstring\tcount\tcount\tcount\tcount\tset[string]\n",
    "'''\n",
    "\n",
    "TIME_STAMP =    28\n",
    "FLOW_ID =       '-'\n",
    "SOURCE_IP =     0\n",
    "SOURCE_PORT =   1\n",
    "DEST_IP =       2\n",
    "DEST_PORT =     3\n",
    "PROTOCOL =      4\n",
    "SERVICE =       13\n",
    "DURATION =      6\n",
    "FWD_BYTES =     7\n",
    "BWD_BYTES =     8\n",
    "CONN_STATE =    5\n",
    "LOCAL_ORIG =    '-'\n",
    "LOCAL_RESP =    '-'\n",
    "MISSED =        '-'\n",
    "HISTORY =       '-'\n",
    "ORIG_PKTS =     16\n",
    "ORIG_IP_BYTES = '-'\n",
    "RESP_PKTS =     17\n",
    "RESP_IP_BYTES = '-'\n",
    "TUNNEL_PARENT = '(empty)'\n",
    "\n",
    "with open(INPUT_CSV, \"r\") as data, open(ZEEK_OUTPUT, \"w\") as output:\n",
    "    flows = csv.reader(data)\n",
    "\n",
    "    output.write(ZEEK_HEADER)\n",
    "    \n",
    "    fid = 1\n",
    "\n",
    "    for flow in flows:\n",
    "        time = flow[TIME_STAMP]\n",
    "        flowid = str(fid)\n",
    "        fid += 1\n",
    "        src_ip = flow[SOURCE_IP]\n",
    "        src_port = flow[SOURCE_PORT]\n",
    "        des_ip = flow[DEST_IP]\n",
    "        des_port = flow[DEST_PORT]\n",
    "        if 'x' in flow[DEST_PORT].lower(): # turn hexadecimal into int\n",
    "            des_port = str(int(flow[DEST_PORT], 16))\n",
    "        protocol = flow[PROTOCOL]\n",
    "        service = flow[SERVICE]\n",
    "        duration = flow[DURATION]\n",
    "        fwd_bytes = flow[FWD_BYTES]\n",
    "        bwd_bytes = flow[BWD_BYTES]\n",
    "        conn_state = flow[CONN_STATE]\n",
    "        local_orig = LOCAL_ORIG\n",
    "        local_resp = LOCAL_RESP\n",
    "        missed = MISSED\n",
    "        history = HISTORY\n",
    "        orig_pkts = flow[ORIG_PKTS]\n",
    "        orig_ip_bytes = ORIG_IP_BYTES\n",
    "        resp_pkts = flow[RESP_PKTS]\n",
    "        resp_ip_bytes = RESP_IP_BYTES\n",
    "        tunnel_parent = TUNNEL_PARENT\n",
    "\n",
    "        processed_line = time + \"\\t\" + flowid + \"\\t\" + src_ip + \"\\t\" + src_port + \"\\t\" + des_ip + \"\\t\" + des_port + \"\\t\" + protocol + \"\\t\" + service + \"\\t\" + duration + \"\\t\" + fwd_bytes + \"\\t\" + bwd_bytes + \"\\t\" + conn_state + \"\\t\" + local_orig + \"\\t\" + local_resp + \"\\t\" + missed + \"\\t\" + history + \"\\t\" + orig_pkts + \"\\t\" + orig_ip_bytes + \"\\t\" + resp_pkts + \"\\t\" + resp_ip_bytes + \"\\t\" + tunnel_parent + \"\\n\"\n",
    "\n",
    "        output.write(processed_line)\n",
    "\n",
    "    output.write(\"#close\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "The above code may be used to convert testing sets to zeek flows as well. When converted, run Slips in testing mode and it will output the labelled flows in sqlite database format. For testing, \"UNSWNB15_assets\\UNSW-NB15_2.csv\" will be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating UNSWNB15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of flows compared:  513598\n",
      "TN:  431293\n",
      "FN:  23190\n",
      "TP:  0\n",
      "FP:  59115\n",
      "Accuracy:  0.839748207742242\n",
      "Recall:  0.0\n",
      "Precision:  0.0\n"
     ]
    }
   ],
   "source": [
    "ORIGINAL_FLOWS = \"UNSWNB15_assets/UNSW-NB15_2.csv\"\n",
    "OUTPUT_FLOWS = \"UNSWNB15_assets/UNSW-NB15_2_output.csv\"\n",
    "\n",
    "with open(ORIGINAL_FLOWS, \"r\") as original, open(OUTPUT_FLOWS, \"r\") as processed:\n",
    "    original_flows = csv.reader(original)\n",
    "    processed_flows = csv.reader(processed)\n",
    "\n",
    "    # counter for number of flows compared\n",
    "    count = 0\n",
    "\n",
    "    # statistics\n",
    "    fp = 0\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    tn = 0  \n",
    "\n",
    "    for original_flow, processed_flow in zip(original_flows, processed_flows):\n",
    "\n",
    "        classification = \"ERROR\" #default\n",
    "\n",
    "        if(original_flow[-1] == \"0\" and processed_flow[2].upper() == \"BENIGN\"): #TN\n",
    "            classification = \"TN\"\n",
    "            tn += 1\n",
    "        if(original_flow[-1] == \"0\" and processed_flow[2].upper() != \"BENIGN\"): #FP\n",
    "            classification = \"FP\"\n",
    "            fp += 1\n",
    "        if(original_flow[-1] != \"0\" and processed_flow[2].upper() == \"BENIGN\"): #FN\n",
    "            classification = \"FN\"\n",
    "            fn += 1\n",
    "        if(original_flow[-1] != \"0\" and processed_flow[2].upper() != \"BENIGN\"): #TP\n",
    "            classification = \"TP\"\n",
    "            tp += 1\n",
    "\n",
    "        #print(processed_flow[2], original_flow[-1], classification)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    # print results\n",
    "    print(\"Number of flows compared: \", count)\n",
    "    print(\"TN: \", tn)\n",
    "    print(\"FN: \", fn)\n",
    "    print(\"TP: \", tp)\n",
    "    print(\"FP: \", fp)\n",
    "    print(\"Accuracy: \", (tp + tn) / count)\n",
    "    print(\"Recall: \", tp / (tp + fn))\n",
    "    print(\"Precision: \", tp / (tp + fp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing BoT-IOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "This dataset is structured similarly to UNSWNB15 so the same processing can be applied to create the required zeek/bro flows to test and train the machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "TRAINING_SET = \"BOTIOT_assets/UNSW_2018_IoT_Botnet_Full5pc_4.csv\"\n",
    "TRAIN_MALICIOUS = \"BOTIOT_assets/botiot_train_malicious.csv\"\n",
    "TRAIN_BENIGN = \"BOTIOT_assets/botiot_train_benign.csv\"\n",
    "\n",
    "with open(TRAINING_SET, \"r\") as train, open(TRAIN_MALICIOUS, \"w\", newline=\"\") as mal, open(TRAIN_BENIGN, \"w\", newline=\"\") as ben:\n",
    "    train_in = csv.reader(train)\n",
    "    ben_out = csv.writer(ben)\n",
    "    mal_out = csv.writer(mal)\n",
    "\n",
    "    next(train_in)\n",
    "\n",
    "    for flow in train_in:\n",
    "        if flow[-3] == \"0\":\n",
    "            ben_out.writerow(flow)\n",
    "        else:\n",
    "            mal_out.writerow(flow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, convert the training or testing flows into zeek/bro flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "#INPUT_CSV = \"BOTIOT_assets/botiot_train_malicious.csv\" # CHANGE THIS AS REQUIRED\n",
    "#ZEEK_OUTPUT = \"BOTIOT_assets/malicious_conn.log\" # CHANGE THIS AS REQUIRED\n",
    "\n",
    "INPUT_CSV = \"BOTIOT_assets/botiot_train_malicious.csv\" # CHANGE THIS AS REQUIRED\n",
    "ZEEK_OUTPUT = \"BOTIOT_assets/malicious_conn.log\" # CHANGE THIS AS REQUIRED\n",
    "\n",
    "# This is a standard header of all Zeek conn.log flow files. Note that the values for #open and #path does not matter.\n",
    "ZEEK_HEADER ='''#separator \\\\x09\n",
    "#set_separator\t,\n",
    "#empty_field\t(empty)\n",
    "#unset_field\t-\n",
    "#path\tconn\n",
    "#open\n",
    "#fields\tts\tuid\tid.orig_h\tid.orig_p\tid.resp_h\tid.resp_p\tproto\tservice\tduration\torig_bytes\tresp_bytes\tconn_state\tlocal_orig\tlocal_resp\tmissed_bytes\thistory\torig_pkts\torig_ip_bytes\tresp_pkts\tresp_ip_bytes\ttunnel_parents\n",
    "#types\ttime\tstring\taddr\tport\taddr\tport\tenum\tstring\tinterval\tcount\tcount\tstring\tbool\tbool\tcount\tstring\tcount\tcount\tcount\tcount\tset[string]\n",
    "'''\n",
    "\n",
    "TIME_STAMP =    1\n",
    "FLOW_ID =       '-'\n",
    "SOURCE_IP =     6\n",
    "SOURCE_PORT =   7\n",
    "DEST_IP =       8\n",
    "DEST_PORT =     9\n",
    "PROTOCOL =      4\n",
    "SERVICE =       '-'\n",
    "DURATION =      16\n",
    "FWD_BYTES =     24\n",
    "BWD_BYTES =     25\n",
    "CONN_STATE =    12\n",
    "LOCAL_ORIG =    '-'\n",
    "LOCAL_RESP =    '-'\n",
    "MISSED =        '-'\n",
    "HISTORY =       '-'\n",
    "ORIG_PKTS =     22\n",
    "ORIG_IP_BYTES = '-'\n",
    "RESP_PKTS =     23\n",
    "RESP_IP_BYTES = '-'\n",
    "TUNNEL_PARENT = '(empty)'\n",
    "\n",
    "with open(INPUT_CSV, \"r\") as data, open(ZEEK_OUTPUT, \"w\") as output:\n",
    "    flows = csv.reader(data)\n",
    "\n",
    "    output.write(ZEEK_HEADER)\n",
    "    \n",
    "    fid = 1\n",
    "\n",
    "    for flow in flows:\n",
    "        time = flow[TIME_STAMP]\n",
    "        flowid = str(fid)\n",
    "        fid += 1\n",
    "        src_ip = flow[SOURCE_IP]\n",
    "        src_port = flow[SOURCE_PORT]\n",
    "        if 'x' in flow[SOURCE_PORT].lower(): # turn hexadecimal into int\n",
    "            src_port = str(int(flow[SOURCE_PORT], 16))\n",
    "        des_ip = flow[DEST_IP]\n",
    "        des_port = flow[DEST_PORT]\n",
    "        if 'x' in flow[DEST_PORT].lower(): # turn hexadecimal into int\n",
    "            des_port = str(int(flow[DEST_PORT], 16))\n",
    "        protocol = flow[PROTOCOL]\n",
    "        service = SERVICE\n",
    "        duration = flow[DURATION]\n",
    "        fwd_bytes = flow[FWD_BYTES]\n",
    "        bwd_bytes = flow[BWD_BYTES]\n",
    "        conn_state = flow[CONN_STATE]\n",
    "        local_orig = LOCAL_ORIG\n",
    "        local_resp = LOCAL_RESP\n",
    "        missed = MISSED\n",
    "        history = HISTORY\n",
    "        orig_pkts = flow[ORIG_PKTS]\n",
    "        orig_ip_bytes = ORIG_IP_BYTES\n",
    "        resp_pkts = flow[RESP_PKTS]\n",
    "        resp_ip_bytes = RESP_IP_BYTES\n",
    "        tunnel_parent = TUNNEL_PARENT\n",
    "\n",
    "        processed_line = time + \"\\t\" + flowid + \"\\t\" + src_ip + \"\\t\" + src_port + \"\\t\" + des_ip + \"\\t\" + des_port + \"\\t\" + protocol + \"\\t\" + service + \"\\t\" + duration + \"\\t\" + fwd_bytes + \"\\t\" + bwd_bytes + \"\\t\" + conn_state + \"\\t\" + local_orig + \"\\t\" + local_resp + \"\\t\" + missed + \"\\t\" + history + \"\\t\" + orig_pkts + \"\\t\" + orig_ip_bytes + \"\\t\" + resp_pkts + \"\\t\" + resp_ip_bytes + \"\\t\" + tunnel_parent + \"\\n\"\n",
    "\n",
    "        output.write(processed_line)\n",
    "\n",
    "    output.write(\"#close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
