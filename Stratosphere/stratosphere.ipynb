{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing CICIDS2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the labelled flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Function to convert scientific notation to decimal\n",
    "def scientific_to_decimal(value):\n",
    "    try:\n",
    "        # Check if the value is in scientific notation (contains 'e' or 'E')\n",
    "        if 'e+' in value or 'E+' in value:\n",
    "            decimal_value = int(float(value))\n",
    "            return decimal_value\n",
    "        else:\n",
    "            # Return the original value if it's not in scientific notation\n",
    "            return value\n",
    "    except ValueError:\n",
    "        # If it's not a valid float, return the original value\n",
    "        return value\n",
    "\n",
    "# Input CSV file and output cleaned CSV file\n",
    "INPUT_CSV_FILE = \"Thursday-WorkingHours-Full.csv\"\n",
    "OUTPUT_CSV_FILE = \"Thursday-WorkingHours-Full_cleaned.csv\"\n",
    "\n",
    "# Open the input and output CSV files\n",
    "with open(INPUT_CSV_FILE, \"r\") as infile, open(OUTPUT_CSV_FILE, \"w\", newline=\"\") as outfile:\n",
    "    reader = csv.reader(infile)\n",
    "    writer = csv.writer(outfile)\n",
    "    \n",
    "    for row in reader:\n",
    "        # Apply the scientific_to_decimal function to fields in scientific notation\n",
    "        cleaned_row = [scientific_to_decimal(field) for field in row]\n",
    "        writer.writerow(cleaned_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert labelled flows into Zeek/Bro conn.log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import csv\n",
    "\n",
    "CICIDS_LABELLED = \"Thursday-WorkingHours-Full_cleaned.csv\"\n",
    "ZEEK_OUTPUT = \"conn.log\"\n",
    "\n",
    "# This is a standard header of all Zeek conn.log flow files. Note that the values for #open and #path does not matter.\n",
    "ZEEK_HEADER ='''#separator \\\\x09\n",
    "#set_separator\t,\n",
    "#empty_field\t(empty)\n",
    "#unset_field\t-\n",
    "#path\tconn\n",
    "#open\n",
    "#fields\tts\tuid\tid.orig_h\tid.orig_p\tid.resp_h\tid.resp_p\tproto\tservice\tduration\torig_bytes\tresp_bytes\tconn_state\tlocal_orig\tlocal_resp\tmissed_bytes\thistory\torig_pkts\torig_ip_bytes\tresp_pkts\tresp_ip_bytes\ttunnel_parents\n",
    "#types\ttime\tstring\taddr\tport\taddr\tport\tenum\tstring\tinterval\tcount\tcount\tstring\tbool\tbool\tcount\tstring\tcount\tcount\tcount\tcount\tset[string]\n",
    "'''\n",
    "\n",
    "# conn.log fields mapped to CICIDS flows file\n",
    "# for fields that do not have a mapping, a default value is set\n",
    "TIME_STAMP =    6\n",
    "FLOW_ID =       0\n",
    "SOURCE_IP =     1\n",
    "SOURCE_PORT =   2\n",
    "DEST_IP =       3\n",
    "DEST_PORT =     4\n",
    "PROTOCOL =      5\n",
    "SERVICE =       13\n",
    "DURATION =      7\n",
    "FWD_BYTES =     10\n",
    "BWD_BYTES =     11\n",
    "CONN_STATE =    '-'\n",
    "LOCAL_ORIG =    '-'\n",
    "LOCAL_RESP =    '-'\n",
    "MISSED =        '-'\n",
    "HISTORY =       '-'\n",
    "ORIG_PKTS =     8\n",
    "ORIG_IP_BYTES = '-'\n",
    "RESP_PKTS =     9\n",
    "RESP_IP_BYTES = '-'\n",
    "TUNNEL_PARENT = '(empty)'\n",
    "\n",
    "# protocol mapping for CICIDS2017\n",
    "PROTOCOL_DICT = {\n",
    "    1: \"icmp\",\n",
    "    2: \"igmp\",\n",
    "    3: \"ggp\",\n",
    "    6: \"tcp\",\n",
    "    8: \"egp\",\n",
    "    9: \"igp\",\n",
    "    17: \"udp\",\n",
    "    20: \"hmp\",\n",
    "    27: \"rdp\",\n",
    "    41: \"ipv6\",\n",
    "    46: \"rsvp\",\n",
    "    47: \"gre\",\n",
    "    50: \"esp\",\n",
    "    51: \"ah\",\n",
    "    58: \"ipv6-icmp\",\n",
    "    59: \"ipv6-nonxt\",\n",
    "    88: \"eigrp\",\n",
    "    89: \"ospf\",\n",
    "    115: \"l2tp\",\n",
    "    132: \"sctp\",\n",
    "    139: \"netbios\",\n",
    "    143: \"imap\",\n",
    "    161: \"snmp\",\n",
    "    179: \"bgp\"\n",
    "}\n",
    "\n",
    "with open(CICIDS_LABELLED, \"r\") as data, open(ZEEK_OUTPUT, \"w\") as output:\n",
    "    labelled_flows = csv.reader(data)\n",
    "    next(labelled_flows) #skip header\n",
    "    \n",
    "    output.write(ZEEK_HEADER)\n",
    "\n",
    "    fid = 1\n",
    "\n",
    "    for flow in labelled_flows:\n",
    "        time = str(datetime.strptime(flow[TIME_STAMP], \"%d/%m/%Y %H:%M\").timestamp()) # Thursday flows don't include seconds\n",
    "        #time = str(datetime.strptime(flow[TIME_STAMP], \"%d/%m/%Y %H:%M:%S\").timestamp()) # Monday flows do include seconds\n",
    "        #flowid = flow[FLOW_ID]\n",
    "        flowid = str(fid)\n",
    "        fid += 1\n",
    "        src_ip = flow[SOURCE_IP]\n",
    "        src_port = flow[SOURCE_PORT]\n",
    "        des_ip = flow[DEST_IP]\n",
    "        des_port = flow[DEST_PORT]\n",
    "        protocol = PROTOCOL_DICT.get(int(flow[PROTOCOL]), 'tcp')\n",
    "        service = SERVICE\n",
    "        duration = format(int(flow[DURATION]) / 1000000, \".6f\") \n",
    "        fwd_bytes = flow[FWD_BYTES]\n",
    "        bwd_bytes = flow[BWD_BYTES]\n",
    "        conn_state = CONN_STATE\n",
    "        local_orig = LOCAL_ORIG\n",
    "        local_resp = LOCAL_RESP\n",
    "        missed = MISSED\n",
    "        history = HISTORY\n",
    "        orig_pkts = flow[ORIG_PKTS]\n",
    "        orig_ip_bytes = ORIG_IP_BYTES\n",
    "        resp_pkts = flow[RESP_PKTS]\n",
    "        resp_ip_bytes = RESP_IP_BYTES\n",
    "        tunnel_parent = TUNNEL_PARENT\n",
    "\n",
    "        processed_line = time + \"\\t\" + flowid + \"\\t\" + src_ip + \"\\t\" + src_port + \"\\t\" + des_ip + \"\\t\" + des_port + \"\\t\" + protocol + \"\\t\" + service + \"\\t\" + duration + \"\\t\" + fwd_bytes + \"\\t\" + bwd_bytes + \"\\t\" + conn_state + \"\\t\" + local_orig + \"\\t\" + local_resp + \"\\t\" + missed + \"\\t\" + history + \"\\t\" + orig_pkts + \"\\t\" + orig_ip_bytes + \"\\t\" + resp_pkts + \"\\t\" + resp_ip_bytes + \"\\t\" + tunnel_parent + \"\\n\"\n",
    "\n",
    "        output.write(processed_line)\n",
    "\n",
    "    output.write(\"#close\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test or train\n",
    "\n",
    "When conn.log has been obtained, copy the file to the straosphere docker container to test the model. An sqlite database will be output with labelled flows and it can be exported to a csv. This can be compared directly with the original labelled flows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate CICIDS2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of flows compared:  458968\n",
      "TN:  430133\n",
      "FN:  2117\n",
      "TP:  99\n",
      "FP:  26619\n",
      "Accuracy:  0.9373899705426086\n",
      "Recall:  0.044675090252707585\n",
      "Precision:  0.0037053671682012127\n"
     ]
    }
   ],
   "source": [
    "ORIGINAL_FLOWS = \"Thursday-WorkingHours-Full_cleaned.csv\"\n",
    "OUTPUT_FLOWS = \"Thursday-Output-Full.csv\"\n",
    "\n",
    "with open(ORIGINAL_FLOWS, \"r\") as original, open(OUTPUT_FLOWS, \"r\") as processed:\n",
    "    original_flows = csv.reader(original)\n",
    "    processed_flows = csv.reader(processed)\n",
    "\n",
    "    next(original_flows) #skip header\n",
    "\n",
    "    # counter for number of flows compared\n",
    "    count = 0\n",
    "\n",
    "    # statistics\n",
    "    fp = 0\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    tn = 0  \n",
    "\n",
    "    for original_flow, processed_flow in zip(original_flows, processed_flows):\n",
    "\n",
    "        classification = \"ERROR\" #default\n",
    "\n",
    "        if(original_flow[-1] == \"BENIGN\" and processed_flow[2].upper() == \"BENIGN\"): #TN\n",
    "            classification = \"TN\"\n",
    "            tn += 1\n",
    "        if(original_flow[-1] == \"BENIGN\" and processed_flow[2].upper() != \"BENIGN\"): #FP\n",
    "            classification = \"FP\"\n",
    "            fp += 1\n",
    "        if(original_flow[-1] != \"BENIGN\" and processed_flow[2].upper() == \"BENIGN\"): #FN\n",
    "            classification = \"FN\"\n",
    "            fn += 1\n",
    "        if(original_flow[-1] != \"BENIGN\" and processed_flow[2].upper() != \"BENIGN\"): #TP\n",
    "            classification = \"TP\"\n",
    "            tp += 1\n",
    "\n",
    "        # print(processed_flow[0], original_flow[-1], processed_flow[2].upper(), classification)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    # print results\n",
    "    print(\"Number of flows compared: \", count)\n",
    "    print(\"TN: \", tn)\n",
    "    print(\"FN: \", fn)\n",
    "    print(\"TP: \", tp)\n",
    "    print(\"FP: \", fp)\n",
    "    print(\"Accuracy: \", (tp + tn) / count)\n",
    "    print(\"Recall: \", tp / (tp + fn))\n",
    "    print(\"Precision: \", tp / (tp + fp))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
